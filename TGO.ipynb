{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Graph Optimization in PyTorch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates optimization rules for computation graphs in PyTorch. These rules identify and eliminate redundant operations, improving computational efficiency without changing the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule 1: Shape-Changing Operations Before Reductions\n",
    "\n",
    "Rule Explanation:\n",
    "\n",
    "Shape-changing operations (e.g., reshape, transpose) applied before reductions (e.g., sum, mean) are redundant because reductions operate on all elements, regardless of shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Optimization:\n",
      "Result: 80.32412719726562\n",
      "Time take for computation: 0.3600120544433594 ms\n",
      "\n",
      "After Optimization:\n",
      "Result: 80.32412719726562\n",
      "Time take for computation: 0.06413459777832031 ms\n"
     ]
    }
   ],
   "source": [
    "# Input tensor\n",
    "x = torch.randn(64, 64)\n",
    "\n",
    "# Redundant reshape before reduction\n",
    "print(\"Before Optimization:\")\n",
    "start = time()\n",
    "x_reshaped = x.view(-1, 64*64)  # Change shape to (1, 16)\n",
    "result = x_reshaped.sum()\n",
    "end = time()\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Time take for computation: {(end - start)*1000} ms\")\n",
    "\n",
    "# Optimized version\n",
    "print(\"\\nAfter Optimization:\")\n",
    "start  = time()\n",
    "result_optimized = x.sum()\n",
    "end = time()\n",
    "print(f\"Result: {result_optimized}\")\n",
    "print(f\"Time take for computation: {(end - start)*1000} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The view operation changes the shape to (1, 64*64), but the sum operation ignores this shape.\n",
    "- Directly applying sum without reshaping eliminates unnecessary overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule 2: Redundant Element-Wise Operations\n",
    "\n",
    "Rule Explanation:\n",
    "- Consecutive element-wise operations (e.g., addition, multiplication, exponentiation) can be fused into a single operation to reduce execution time and memory overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Optimization:\n",
      "tensor([[ 3.8547, 10.6835,  3.8553,  2.1362],\n",
      "        [ 1.9351,  4.4751,  9.1259,  8.2489],\n",
      "        [ 5.7428,  3.9397,  4.6948,  3.8339],\n",
      "        [ 1.3104,  4.2059,  8.3593,  1.2723]])\n",
      "Time take for computation: 0.5605220794677734 ms\n",
      "\n",
      "After Optimization:\n",
      "tensor([[ 3.8547, 10.6835,  3.8553,  2.1362],\n",
      "        [ 1.9351,  4.4751,  9.1259,  8.2489],\n",
      "        [ 5.7428,  3.9397,  4.6948,  3.8339],\n",
      "        [ 1.3104,  4.2059,  8.3593,  1.2723]])\n",
      "Time take for computation: 0.07748603820800781 ms\n"
     ]
    }
   ],
   "source": [
    "# Input tensor\n",
    "x = torch.randn(4, 4)\n",
    "\n",
    "# Redundant element-wise operations\n",
    "print(\"Before Optimization:\")\n",
    "start  = time()\n",
    "y = torch.exp(x)\n",
    "z = y * 2\n",
    "result = z + 1\n",
    "end = time()\n",
    "print(result)\n",
    "print(f\"Time take for computation: {(end - start)*1000} ms\")\n",
    "\n",
    "# Optimized version\n",
    "print(\"\\nAfter Optimization:\")\n",
    "start  = time()\n",
    "result_optimized = torch.exp(x) * 2 + 1\n",
    "end = time()\n",
    "print(result_optimized)\n",
    "print(f\"Time take for computation: {(end - start)*1000} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule 3: Consecutive Shape Transformations\n",
    "\n",
    "Rule Explanation:\n",
    "- Consecutive shape-transforming operations (e.g., reshape, permute, transpose) can often be combined into a single transformation to simplify the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Optimization:\n",
      "torch.Size([4, 6])\n",
      "Time take for computation: 0.46706199645996094 ms\n",
      "\n",
      "After Optimization:\n",
      "torch.Size([4, 6])\n",
      "Time take for computation: 0.1068115234375 ms\n"
     ]
    }
   ],
   "source": [
    "# Input tensor\n",
    "x = torch.randn(2, 3, 4)\n",
    "\n",
    "# Consecutive transformations\n",
    "print(\"Before Optimization:\")\n",
    "start  = time()\n",
    "x_transformed = x.permute(2, 0, 1).reshape(4, 6)\n",
    "end = time()\n",
    "print(x_transformed.shape)\n",
    "print(f\"Time take for computation: {(end - start)*1000} ms\")\n",
    "\n",
    "\n",
    "\n",
    "# Optimized version\n",
    "print(\"\\nAfter Optimization:\")\n",
    "start  = time()\n",
    "x_optimized = x.permute(2, 0, 1).reshape(4, 6)\n",
    "end = time()\n",
    "print(x_optimized.shape)\n",
    "print(f\"Time take for computation: {(end - start)*1000} ms\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule 4: Eliminating No-Op Transformations\n",
    "\n",
    "Rule Explanation:\n",
    "- No-op transformations (e.g., reshaping to the same shape or transposing a tensor without changing axes) can be removed as they do not alter the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Optimization:\n",
      "tensor(0.4144)\n",
      "Time take for computation: 0.9541511535644531 ms\n",
      "\n",
      "After Optimization:\n",
      "tensor(0.4144)\n",
      "Time take for computation: 0.059604644775390625 ms\n"
     ]
    }
   ],
   "source": [
    "# Input tensor\n",
    "x = torch.randn(4, 4)\n",
    "\n",
    "# No-op reshape\n",
    "print(\"Before Optimization:\")\n",
    "start  = time()\n",
    "x_no_op = x.view(4, 4)\n",
    "result = x_no_op.mean()\n",
    "end = time()\n",
    "print(result)\n",
    "print(f\"Time take for computation: {(end - start)*1000} ms\")\n",
    "\n",
    "\n",
    "# Optimized version\n",
    "print(\"\\nAfter Optimization:\")\n",
    "start  = time()\n",
    "result_optimized = x.mean()\n",
    "end = time()\n",
    "print(result_optimized)\n",
    "print(f\"Time take for computation: {(end - start)*1000} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The view operation in the first case does nothing since the shape remains unchanged.\n",
    "- Removing it leads to the same result while eliminating unnecessary steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule 5: Simplifying Reductions\n",
    "\n",
    "Rule Explanation:\n",
    "- Reductions applied consecutively or redundantly can often be simplified into a single reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Optimization:\n",
      "tensor(-0.0030)\n",
      "Time take for computation: 22.881269454956055 ms\n",
      "\n",
      "After Optimization:\n",
      "tensor(-0.0002)\n",
      "Time take for computation: 0.1373291015625 ms\n"
     ]
    }
   ],
   "source": [
    "# Input tensor\n",
    "x = torch.randn(4, 4)\n",
    "\n",
    "# Redundant reductions\n",
    "print(\"Before Optimization:\")\n",
    "start  = time()\n",
    "result = x.sum().mean()\n",
    "end = time()\n",
    "print(result)\n",
    "print(f\"Time take for computation: {(end - start)*1000} ms\")\n",
    "\n",
    "\n",
    "# Optimized version\n",
    "print(\"\\nAfter Optimization:\")\n",
    "start  = time()\n",
    "result_optimized = x.mean()\n",
    "end = time()\n",
    "print(result_optimized)\n",
    "print(f\"Time take for computation: {(end - start)*1000} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the first case, the sum is followed by a mean, which could be combined directly as a mean operation.\n",
    "- This simplification reduces overhead without changing the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "By applying these optimization rules, computation graphs in PyTorch can be simplified significantly. This leads to faster execution, reduced memory usage, and cleaner graph representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_compiler_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
